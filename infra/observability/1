===== ./base/policy-reporter-servicemonitor.yaml =====
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: policy-reporter
  namespace: ns-observability
  labels:
    release: prometheus
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: policy-reporter
  namespaceSelector:
    matchNames:
      - observability
  endpoints:
    - port: http
      interval: 30s

===== ./base/kustomization.yaml =====
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - helmrepository-prometheus-community.yaml
  - helmrepository-grafana.yaml
  - releases/kube-prometheus-stack
  - releases/grafana/grafana-helmrelease.yaml           # <---- ajoute Ã§a
  - policy-reporter-servicemonitor.yaml
  - kube-components-services
  - kube-components-rbac
  - grafana-admin-secret.yaml
  - releases/promtail
  - releases/victoria-metrics

===== ./base/grafana-admin-secret.yaml =====
apiVersion: v1
kind: Secret
metadata:
  name: grafana-admin
  namespace: ns-observability
type: Opaque
stringData:
  admin-user: admin
  admin-password: changeme

===== ./base/kube-components-services/kube-controller-manager-service.yaml =====
apiVersion: v1
kind: Service
metadata:
  name: kube-controller-manager
  namespace: kube-system
  labels:
    component: kube-controller-manager
spec:
  selector:
    component: kube-controller-manager
  ports:
    - name: https-metrics
      port: 10257
      targetPort: 10257
      protocol: TCP
  clusterIP: None  # Impo

===== ./base/kube-components-services/kustomization.yaml =====
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - kube-scheduler-service.yaml
  - kube-controller-manager-servicemonitor.yaml
  - kube-controller-manager-service.yaml
  - kubelet-service.yaml                             # â† ajout
  - kube-scheduler-servicemonitor.yaml
  - kubelet-servicemonitor.yaml 
  - etcd-service.yaml
  - etcd-servicemonitor.yaml

===== ./base/kube-components-services/kube-scheduler-servicemonitor.yaml =====
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: kube-scheduler-servicemonitor
  namespace: ns-observability
  labels:
    release: kube-prometheus-stack  # â† indispensable si Prometheus l'attend
    kustomize.toolkit.fluxcd.io/name: observability
    kustomize.toolkit.fluxcd.io/namespace: flux-system
spec:
  endpoints:
    - port: https-metrics              # â† correspond Ã  `kube-scheduler-service.yaml`
      scheme: https
      interval: 30s
      tlsConfig:
        insecureSkipVerify: true
      relabelings:
        - action: replace
          sourceLabels: [__meta_kubernetes_service_name]
          targetLabel: job
          replacement: kube-scheduler
  namespaceSelector:
    matchNames:
      - kube-system
  selector:
    matchLabels:
      app: kube-scheduler       # â† correspond au Service exposant 10259

===== ./base/kube-components-services/etcd-service.yaml =====
# ./infra/observability/base/etcd-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: etcd
  namespace: kube-system
  labels:
    k8s-app: etcd
spec:
  selector:
    component: etcd
  ports:
    - name: metrics
      port: 2381
      targetPort: 2381

===== ./base/kube-components-services/kubelet-servicemonitor.yaml =====
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: kubelet
  namespace: ns-observability
  labels:
    release: prometheus  # ou kube-prometheus-stack
spec:
  selector:
    matchLabels:
      k8s-app: kubelet
  namespaceSelector:
    matchNames:
      - kube-system
  endpoints:
    - port: cadvisor
      scheme: https
      path: /metrics/cadvisor
      bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
      tlsConfig:
        insecureSkipVerify: true
      interval: 30s
    - port: http-metrics
      scheme: https
      path: /metrics
      bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
      tlsConfig:
        insecureSkipVerify: true
      interval: 30s

===== ./base/kube-components-services/etcd-servicemonitor.yaml =====
# ./infra/observability/base/etcd-servicemonitor.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: etcd
  namespace: ns-observability
  labels:
    release: kube-prometheus-stack
spec:
  selector:
    matchLabels:
      k8s-app: etcd
  namespaceSelector:
    matchNames:
      - kube-system
  endpoints:
    - port: metrics
      path: /metrics
      interval: 30s

===== ./base/kube-components-services/kubelet-service.yaml =====
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: kubelet-servicemonitor  
  namespace: ns-observability
  labels:
    release: prometheus  # ou kube-prometheus-stack selon ton chart
spec:
  endpoints:
    - port: cadvisor
      scheme: https
      path: /metrics/cadvisor
      bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
      tlsConfig:
        insecureSkipVerify: true
      interval: 30s
    - port: http-metrics
      scheme: https
      path: /metrics
      bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
      tlsConfig:
        insecureSkipVerify: true
      interval: 30s
  selector:
    matchLabels:
      k8s-app: kubelet
  namespaceSelector:
    matchNames:
      - kube-system

===== ./base/kube-components-services/kube-controller-manager-servicemonitor.yaml =====
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: kube-controller-manager
  namespace: ns-observability
  labels:
    release: prometheus
spec:
  selector:
    matchLabels:
      component: kube-controller-manager
  namespaceSelector:
    matchNames:
      - kube-system
  endpoints:
    - port: https-metrics
      scheme: https
      tlsConfig:
        insecureSkipVerify: true
      interval: 30s

===== ./base/kube-components-services/kube-scheduler-service.yaml =====
apiVersion: v1
kind: Service
metadata:
  name: kube-scheduler
  namespace: kube-system
  labels:
    k8s-app: kube-scheduler
    app: kube-scheduler
    component: kube-scheduler
    tier: control-plane
    release: kube-prometheus-stack
spec:
  clusterIP: None
  ports:
    - name: https-metrics
      port: 10259
      protocol: TCP
      targetPort: 10259
  selector:
    component: kube-scheduler
    tier: control-plane

===== ./base/kube-components-rbac/kustomization.yaml =====
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - rbacprometheus.yaml
  - kube-scheduler-metrics-reader.yaml

===== ./base/kube-components-rbac/kube-scheduler-metrics-reader.yaml =====
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: kube-scheduler-metrics-reader
rules:
  - nonResourceURLs: ["/metrics"]
    verbs: ["get"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: kube-scheduler-metrics-reader-binding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: kube-scheduler-metrics-reader
subjects:
  - kind: User
    name: system:anonymous
    apiGroup: rbac.authorization.k8s.io

===== ./base/kube-components-rbac/rbacprometheus.yaml =====
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: prometheus
rules:
  - apiGroups: [""]
    resources:
      - nodes
      - nodes/metrics
      - services
      - endpoints
      - pods
    verbs: ["get", "list", "watch"]
  - nonResourceURLs: ["/metrics"]
    verbs: ["get"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: prometheus
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: prometheus
subjects:
  - kind: ServiceAccount
    name: prometheus-kube-prometheus-stack-prometheus
    namespace: monitoring

===== ./base/helmrepository-grafana.yaml =====
apiVersion: source.toolkit.fluxcd.io/v1
kind: HelmRepository
metadata:
  name: helmrepo-grafana
  namespace: flux-system
spec:
  url: https://grafana.github.io/helm-charts
  interval: 1h

===== ./base/helmrepository-prometheus-community.yaml =====
apiVersion: source.toolkit.fluxcd.io/v1
kind: HelmRepository
metadata:
  name: helmrepo-prometheus-community
  namespace: flux-system
spec:
  url: https://prometheus-community.github.io/helm-charts
  interval: 1h

===== ./base/releases/grafana/grafana-helmrelease.yaml =====
# infra/observability/overlays/lab/grafana-helmrelease.yaml
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: grafana
  namespace: ns-observability
spec:
  interval: 10m
  chart:
    spec:
      chart: grafana
      version: "9.4.5"
      sourceRef:
        kind: HelmRepository
        name: grafana
        namespace: flux-system
  install:
    createNamespace: true
    remediation: { retries: 3 }
    timeout: 10m
  upgrade:
    remediation: { retries: 3 }

  values:
    #####################################
    # Service + Ingress (DNS-01 recommandÃ©)
    #####################################
    service:
      type: NodePort
      nodePort: 30080

    ingress:
      enabled: false
      # âš ï¸ dÃ©commente si tu utilises l'annotation plutÃ´t que spec.ingressClassName
      # annotations:
      #   kubernetes.io/ingress.class: nginx
      # Pour cert-manager (DNS-01 Cloudflare) : laisse cert-manager gÃ©rer la rÃ©solution DNS
      annotations:
        cert-manager.io/cluster-issuer: letsencrypt-prod-dns
      ingressClassName: nginx
      path: /
      pathType: Prefix
      hosts:
        - grafana.nudger.logo-solutions.fr
      tls:
        - secretName: grafana-tls         # cert-manager va le remplir
          hosts:
            - grafana.nudger.logo-solutions.fr

    #####################################
    # ParamÃ¨tres Grafana
    #####################################
    admin:
      existingSecret: grafana-admin
      userKey: admin-user
      passwordKey: admin-password
    # âš ï¸ En prod, remplace par un Secret existant (grafana-admin) ou SOPS :
    # admin:
    #   existingSecret: grafana-admin
    #   userKey: admin-user
    #   passwordKey: admin-password

    #####################################
    # Datasources (Loki + Prometheus)
    #####################################
    datasources:
      datasources.yaml:
        apiVersion: 1
        datasources:
          - name: Prometheus
            type: prometheus
            access: proxy
            url: http://kube-prometheus-stack-prometheus.observability.svc:9090
            isDefault: true
            uid: PBFA97CFB590B2093
            jsonData:
              timeInterval: 15s
          - name: VictoriaLogs
            uid: victorialogs
            type: victoriametrics-logs-datasource
            access: proxy
            url: http://victorialogs.logging.svc.cluster.local:9428
            isDefault: false
            jsonData:
              tlsSkipVerify: true
    #####################################
    # Dashboards (sidecar qui surveille des ConfigMaps/Secrets)
    #####################################
    sidecar:
      dashboards:
        enabled: true
        SCProvider: true
        # Par dÃ©faut, surveille les ConfigMaps avec label grafana_dashboard="1"
        label: grafana_dashboard
        labelValue: "1"
        searchNamespace: ALL
        folder: /var/lib/grafana/dashboards/Kubernetes
        # Tu peux aussi charger depuis des Secrets :
        # foldersFromFilesStructure: true
        provider:
          # Le chart crÃ©e dÃ©jÃ  ce provider, on force ses options
          foldersFromFilesStructure: false
          # Dossier LOGIQUE dans lâ€™UI Grafana
          folder: "Kubernetes"
          # Et surtout, chemin ABSOLU lu par Grafana
        options:
          path: /var/lib/grafana/dashboards/Kubernetes
    # Exemple de dashboard gÃ©rÃ© par le chart lui-mÃªme (optionnel)
    # Place ton JSON dans un ConfigMap labelisÃ© grafana_dashboard=1 pour itÃ©rer sans toucher ce fichier.
    dashboards:
      default:
        # Un bon point de dÃ©part : NGINX Ingress (officiel)
        nginx-ingress:
          # Remplace par lâ€™UID ou lâ€™URL de ton choix si besoin
          gnetId: 9614
          revision: 1
          datasource: Prometheus

    #####################################
    # Persistence
    #####################################
    persistence:
      enabled: false
      type: pvc
      size: 5Gi
      storageClassName: longhorn-delete
      accessModes: ["ReadWriteOnce"]

    #####################################
    # grafana.ini â€” rÃ©glages utiles
    #####################################
    grafana.ini:
      server:
        root_url: https://grafana.nudger.logo-solutions.fr
        domain: grafana.nudger.logo-solutions.fr
        enable_gzip: true
      security:
        cookie_samesite: lax
      analytics:
        reporting_enabled: false
        check_for_updates: true
      plugins:
        - victoriametrics-logs-datasource@0.4.0
      default:
        timezone: Europe/Paris
      users:
        default_theme: light
        viewers_can_edit: false
      explore:
        logsPanel:
          defaultLogVolumeEnabled: false

    #####################################
    # Plugins (facultatif)
    #####################################
    plugins:
      - victoriametrics-logs-datasource

    #####################################
    # Resources & PodSecurity
    #####################################
    resources:
      requests:
        cpu: 100m
        memory: 256Mi
      limits:
        cpu: 500m
        memory: 512Mi

    podSecurityPolicy:
      enabled: false

    #####################################
    # ServiceMonitor (scrape Grafana par Prometheus Operator)
    #####################################
    serviceMonitor:
      enabled: true
      namespace: ns-observability
      interval: 30s
      labels: {}
      scheme: http
      scrapeTimeout: 10s

    #####################################
    # Tolerations / NodeSelector (optionnel)
    #####################################
    # tolerations:
    #   - key: "node-role.kubernetes.io/master"
    #     operator: "Exists"
    #     effect: "NoSchedule"
    # nodeSelector:
    #   kubernetes.io/os: linux

    #####################################
    # SMTP / Alerting (placeholders)
    #####################################
    # smtp:
    #   enabled: true
    #   host: smtp.example.com:587
    #   user: grafana@example.com
    #   password: "****"
    #   from_address: grafana@example.com
    #   startTLS_policy: opportunistic

===== ./base/releases/grafana/kustomization.yaml =====
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - helmrelease.yaml
  - grafana-helmrelease.yaml

===== ./base/releases/promtail/helmrelease.yaml =====
# infra/observability/base/promtail-helmrelease.yaml
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: promtail
  namespace: ns-logging
spec:
  interval: 5m
  chart:
    spec:
      chart: promtail
      version: "6.x.x"
      sourceRef:
        kind: HelmRepository
        name: grafana
        namespace: flux-system
  values:
    extraVolumes: []
    extraVolumeMounts: []
    extraScrapeConfigs: |
      - job_name: kubernetes-pods
        pipeline_stages: []
        kubernetes_sd_configs:
        - role: pod
        relabel_configs:
        - action: keep
          source_labels: [__meta_kubernetes_pod_container_name]
          regex: .*
        # DROP les fichiers trop anciens => empÃªche les 400 futurs
        ignore_older_than: 6h
    podLabels:
      security.nudger/allow-rw-rootfs: "true"
    podAnnotations:
      security.nudger/justification: "Promtail a besoin d'un positions file en Ã©criture (/run/promtail)."
    # --- SÃ©curitÃ© (non-root) + FS en Ã©criture uniquement oÃ¹ nÃ©cessaire
    podSecurityContext:
      runAsNonRoot: false
      fsGroup: 0
      seccompProfile:
        type: RuntimeDefault
    securityContext: # (contexte CONTAINER dans le chart promtail)
      runAsUser: 0
      runAsGroup: 0
      allowPrivilegeEscalation: false
      readOnlyRootFilesystem: true
      capabilities:
        drop: ["ALL"]
    fullnameOverride: promtail
    rbac:
      create: true
    serviceAccount:
      create: true
      name: promtail
    deployment:
      enabled: false
    daemonset:
      enabled: true
    tolerations:
      - key: "node-role.kubernetes.io/control-plane"
        operator: "Exists"
        effect: "NoSchedule"
      - key: node-role.kubernetes.io/master
        operator: Exists
        effect: NoSchedule
    # promtail lit les logs hÃ´te -> root + hostPath
    # values override (ex: kustomize patch ou values dans HelmRelease)
    config:
      server:
        http_listen_port: 3101
      clients:
        - url: http://victorialogs.logging.svc:9428/insert/loki/api/v1/push
          batchwait: 2s # plus frÃ©quent, paquets plus petits
          batchsize: 1048576 # 1MiB
          backoff_config:
            min_period: 500ms
            max_period: 5s
            max_retries: 10
      positions:
        filename: /run/promtail/positions.yaml
      scrape_configs:
        - job_name: kubernetes-pods
          pipeline_stages: [{cri: {}}]
          kubernetes_sd_configs: [{role: pod}]
          relabel_configs:
            - source_labels: [__meta_kubernetes_pod_uid, __meta_kubernetes_pod_container_name]
              separator: /
              target_label: __path__
              replacement: /var/log/pods/*$1/*/*.log

===== ./base/releases/promtail/kustomization.yaml =====
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - helmrelease.yaml

===== ./base/releases/kube-prometheus-stack/helmrelease.yaml =====
# infra/observability/releases/kube-prometheus-stack/helmrelease.yaml
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: kube-prometheus-stack
  namespace: ns-observability
spec:
  interval: 10m
  chart:
    spec:
      chart: kube-prometheus-stack
      version: "56.0.3"  # ou la version compatible avec ta HelmRepository
      sourceRef:
        kind: HelmRepository
        name: prometheus-community
        namespace: flux-system
  install:
    createNamespace: true
    remediation:
      retries: 3
    timeout: 10m
  upgrade:
    remediation:
      retries: 3
  values:
    alertmanager:
      enabled: true
    grafana:
      enabled: false  # tu as dÃ©jÃ  un Grafana gÃ©rÃ© ailleurs
    kubeApiServer:
      enabled: true
    kubelet:
      enabled: true
    kubeControllerManager:
      enabled: true
    coreDns:
      enabled: true
    kubeScheduler:
      enabled: true
    kubeProxy:
      enabled: true
    prometheus:
      enabled: true
      prometheusSpec:
        serviceMonitorSelectorNilUsesHelmValues: false
        podMonitorSelectorNilUsesHelmValues: false
        ruleSelectorNilUsesHelmValues: false
        serviceMonitorSelector: {}
    prometheusOperator:
      enabled: true

===== ./base/releases/kube-prometheus-stack/kustomization.yaml =====
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - helmrelease.yaml

===== ./base/releases/victoria-metrics/kustomization.yaml =====
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - victoria-metrics-helmrelease.yaml

===== ./base/releases/victoria-metrics/victoria-metrics-helmrelease.yaml =====
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: victoria-metrics
  namespace: ns-logging
spec:
  interval: 10m
  chart:
    spec:
      chart: victoria-metrics-single
      version: "0.24.5"   # prends la derniÃ¨re
      sourceRef:
        kind: HelmRepository
        name: victoria-metrics
        namespace: flux-system
  install:
    replace: true
    createNamespace: true
    remediation:
      retries: 3
      remediateLastFailure: true
  upgrade:
    remediation:
      retries: 3
  values:
    vmagent:
      enabled: false   # si tu nâ€™en as pas besoin
    vmalert:
      enabled: false
    fullnameOverride: victoria-metrics
    replicaCount: 1
    service:
      type: ClusterIP
    server: 
      persistentVolume:
        enabled: false

===== ./base/policy-reporter-helmrelease.yaml =====
---
apiVersion: source.toolkit.fluxcd.io/v1
kind: HelmRepository
metadata:
  name: policy-reporter
  namespace: flux-system
spec:
  interval: 1h
  url: https://kyverno.github.io/policy-reporter
---
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: policy-reporter
  namespace: ns-observability
spec:
  interval: 10m
  chart:
    spec:
      chart: policy-reporter
      version: "2.20.0"  # â—ï¸ Ã  vÃ©rifier selon la version disponible
      sourceRef:
        kind: HelmRepository
        name: policy-reporter
        namespace: flux-system
  install:
    crds: Skip
    remediation:
      retries: 3
  upgrade:
    remediation:
      retries: 3
  values:
    # ðŸ”’ Basique
    namespaceOverride: observability
    fullnameOverride: policy-reporter

    # ðŸš« UI non nÃ©cessaire
    ui:
      enabled: false

    # âœ… Prometheus
    metrics:
      enabled: true

    prometheus:
      enabled: true
      service:
        labels:
          app.kubernetes.io/name: policy-reporter
        enabled: true
        port: 8080
        annotations:
          prometheus.io/scrape: "true"
          prometheus.io/path: "/metrics"
          prometheus.io/port: "8080"

    # ðŸ”• DÃ©sactivation des autres sinks par dÃ©faut (Slack, webhook, UI...)
    loki:
      enabled: false
    webhook:
      enabled: false
    slack:
      enabled: false
    discord:
      enabled: false
    teams:
      enabled: false

    # ðŸ—‚ï¸ Filtrage fin possible
    filterPriorities:
      - critical
      - high
      - medium

===== ./overlays/lab/kustomization.yaml =====
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ../../base
  - ../../dashboard

===== ./overlays/lab/grafana-ingress.yaml =====
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: grafana-ingress
  namespace: ns-observability
  annotations:
    kubernetes.io/ingress.class: nginx
spec:
  ingressClassName: nginx
  rules:
    - host: grafana.nudger.logo-solutions.fr
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: grafana
                port:
                  number: 80

===== ./dashboard/grafana-dashboard-raw-logs.yaml =====
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-dashboard-raw-logs
  namespace: ns-observability
  labels:
    grafana_dashboard: "1"
data:
  raw-logs.json: |
    {
      "title": "Raw Logs Explorer",
      "tags": ["victorialogs"],
      "timezone": "browser",
      "schemaVersion": 41,
      "version": 1,
      "time": { "from": "now-15m", "to": "now" },
      "templating": {
        "list": [
          {
            "name": "namespace",
            "type": "query",
            "datasource": {"type":"victoriametrics-logs-datasource","uid":"victorialogs"},
            "query": {
              "field": "kubernetes.pod_namespace",
              "limit": 100
            },
            "refresh": 2,
            "includeAll": true
          },
          {
            "name": "container",
            "type": "query",
            "datasource": {"type":"victoriametrics-logs-datasource","uid":"victorialogs"},
            "query": {
              "field": "kubernetes.container_name",
              "limit": 100,
              "query": "{kubernetes.pod_namespace=\"$namespace\"}"
            },
            "refresh": 2,
            "includeAll": true
          }
        ]
      },
      "panels": [
        {
          "type": "logs",
          "title": "Raw logs",
          "gridPos": {"x":0,"y":0,"w":24,"h":15},
          "datasource": {"type":"victoriametrics-logs-datasource","uid":"victorialogs"},
          "targets": [
            {
              "refId": "A",
              "expr": "{kubernetes.pod_namespace=\"$namespace\", kubernetes.container_name=~\"$container\"}",
              "queryType": "range"
            }
          ]
        }
      ]
    }

===== ./dashboard/grafana-dashboard-flux-errors.yaml =====
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-dashboard-flux-errors
  namespace: ns-observability
  labels:
    grafana_dashboard: "1"
data:
  flux-errors.json: |
    {
      "title": "Flux â€” Errors (last hour)",
      "description": "Monitor Flux errors via VictoriaLogs",
      "timezone": "browser",
      "schemaVersion": 41,
      "version": 1,
      "tags": ["flux", "victorialogs"],
      "time": { "from": "now-1h", "to": "now" },

      "templating": {
        "list": [
          {
            "type": "datasource",
            "name": "DS_VICTORIALOGS",
            "label": "VictoriaLogs DS",
            "query": "victoriametrics-logs-datasource",
            "current": { "selected": true },
            "hide": 2
          },
          {
            "name": "controller",
            "type": "query",
            "datasource": {"type":"victoriametrics-logs-datasource","uid":"victorialogs"},
            "query": {
              "field": "kubernetes.container_name",
              "limit": 50,
              "query": "{namespace=\"flux-system\"}"
            },
            "refresh": 2,
            "includeAll": true,
            "multi": true
          }
        ]
      },

      "panels": [
        {
          "type": "stat",
          "title": "Flux error lines (last 1h)",
          "gridPos": {"x":0,"y":0,"w":6,"h":6},
          "datasource": {"type":"victoriametrics-logs-datasource","uid":"victorialogs"},
          "options": {"reduceOptions":{"calcs":["lastNotNull"]}},
          "targets": [
            {
              "refId": "A",
              "expr": "{namespace=\"flux-system\", kubernetes.container_name=~\"$controller\"} (error or failed or timeout) _time:1h | stats count() as errors",
              "queryType": "statsRange"
            }
          ]
        },
        {
          "type": "timeseries",
          "title": "Errors by controller (5m buckets)",
          "gridPos": {"x":6,"y":0,"w":18,"h":6},
          "datasource": {"type":"victoriametrics-logs-datasource","uid":"victorialogs"},
          "targets": [
            {
              "refId": "A",
              "expr": "{namespace=\"flux-system\", kubernetes.container_name=~\"$controller\"} (error or failed or timeout) | stats by (_time:$__interval, kubernetes.container_name) count() as errors",
              "queryType": "statsRange"
            }
          ]
        },
        {
          "type": "table",
          "title": "Top error messages (10m)",
          "gridPos": {"x":0,"y":6,"w":12,"h":10},
          "datasource": {"type":"victoriametrics-logs-datasource","uid":"victorialogs"},
          "targets": [
            {
              "refId": "A",
              "expr": "{namespace=\"flux-system\", kubernetes.container_name=~\"$controller\"} (error or failed or timeout) _time:10m | top 20 by (_msg) hits as errors",
              "queryType": "stats"
            }
          ]
        },
        {
          "type": "logs",
          "title": "Raw error logs",
          "gridPos": {"x":12,"y":6,"w":12,"h":10},
          "datasource": {"type":"victoriametrics-logs-datasource","uid":"victorialogs"},
          "targets": [
            {
              "refId": "A",
              "expr": "{namespace=\"flux-system\", kubernetes.container_name=~\"$controller\"} (error or failed or timeout)",
              "queryType": "range"
            }
          ]
        }
      ]
    }

===== ./dashboard/kustomization.yaml =====
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - victoria1.yaml
  - grafana-dashboard-app-exceptions.yaml
  - grafana-dashboard-flux-errors.yaml
  - grafana-dashboard-ingress-5xx.yaml
  - grafana-dashboard-raw-logs.yaml
  - grafana-dashboard-victoriametrics-errors.yaml
  - grafana-dashboard-victoriametrics-overview.yaml
  - grafana-dashboard-victoriametrics-queries.yaml

===== ./dashboard/grafana-dashboard-victoriametrics-queries.yaml =====
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-dashboard-victoriametrics-queries
  namespace: ns-observability
  labels:
    grafana_dashboard: "1"
data:
  victoriametrics-queries.json: |
    {
      "title": "VictoriaMetrics â€” Queries",
      "uid": "vm-queries",
      "tags": ["victoriametrics"],
      "schemaVersion": 39,
      "version": 1,
      "timezone": "browser",
      "time": { "from": "now-1h", "to": "now" },
      "panels": [
        {
          "type": "timeseries",
          "title": "QPS by status code",
          "gridPos": {"x":0,"y":0,"w":12,"h":6},
          "datasource": {"type":"prometheus","uid":"victoriametrics"},
          "targets": [
            { "expr": "sum by (status_code) (rate(vm_http_requests_total[5m]))" }
          ]
        },
        {
          "type": "timeseries",
          "title": "Query latency p99",
          "gridPos": {"x":12,"y":0,"w":12,"h":6},
          "datasource": {"type":"prometheus","uid":"victoriametrics"},
          "targets": [
            { "expr": "histogram_quantile(0.99, sum(rate(vm_request_duration_seconds_bucket[5m])) by (le))" }
          ]
        }
      ]
    }

===== ./dashboard/grafana-dashboard-app-exceptions.yaml =====
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-dashboard-app-exceptions
  namespace: ns-observability
  labels:
    grafana_dashboard: "1"
data:
  app-exceptions.json: |
    {
      "title": "App Exceptions",
      "tags": ["victorialogs","apps"],
      "timezone": "browser",
      "schemaVersion": 41,
      "version": 1,
      "time": { "from": "now-1h", "to": "now" },
      "panels": [
        {
          "type": "stat",
          "title": "Exceptions (last 1h)",
          "gridPos": {"x":0,"y":0,"w":6,"h":6},
          "datasource": {"type":"victoriametrics-logs-datasource","uid":"victorialogs"},
          "targets": [
            {
              "refId": "A",
              "expr": "(exception or error) _time:1h | stats count() as errors",
              "queryType": "statsRange"
            }
          ]
        },
        {
          "type": "table",
          "title": "Top exception messages",
          "gridPos": {"x":6,"y":0,"w":18,"h":8},
          "datasource": {"type":"victoriametrics-logs-datasource","uid":"victorialogs"},
          "targets": [
            {
              "refId": "B",
              "expr": "(exception or error) _time:10m | top 20 by (_msg) hits as errors",
              "queryType": "stats"
            }
          ]
        },
        {
          "type": "logs",
          "title": "Raw exceptions",
          "gridPos": {"x":0,"y":8,"w":24,"h":10},
          "datasource": {"type":"victoriametrics-logs-datasource","uid":"victorialogs"},
          "targets": [
            {
              "refId": "C",
              "expr": "(exception or error)",
              "queryType": "range"
            }
          ]
        }
      ]
    }

===== ./dashboard/grafana-dashboard-ingress-5xx.yaml =====
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-dashboard-ingress-5xx
  namespace: ns-observability
  labels:
    grafana_dashboard: "1"
data:
  ingress-5xx.json: |
    {
      "annotations": {
        "list": [
          {
            "builtIn": 1,
            "datasource": {
              "type": "grafana",
              "uid": "-- Grafana --"
            },
            "enable": true,
            "hide": true,
            "iconColor": "rgba(0, 211, 255, 1)",
            "name": "Annotations & Alerts",
            "type": "dashboard"
          }
        ]
      },
      "editable": true,
      "id": null,
      "panels": [
        {
          "datasource": {
            "type": "victoriametrics-logs-datasource",
            "uid": "victorialogs"
          },
          "gridPos": { "h": 6, "w": 6, "x": 0, "y": 0 },
          "id": 1,
          "options": {
            "colorMode": "value",
            "graphMode": "area",
            "reduceOptions": { "calcs": ["lastNotNull"] },
            "textMode": "auto"
          },
          "targets": [
            {
              "expr": "{namespace=\"ingress-nginx\", container=\"controller\"} status>=500 and status<600 _time:30m | stats count() as errors",
              "queryType": "statsRange",
              "refId": "A"
            }
          ],
          "title": "5xx in last 30m",
          "type": "stat"
        },
        {
          "datasource": {
            "type": "victoriametrics-logs-datasource",
            "uid": "victorialogs"
          },
          "gridPos": { "h": 6, "w": 18, "x": 6, "y": 0 },
          "id": 2,
          "options": {
            "legend": { "showLegend": true },
            "tooltip": { "mode": "single" }
          },
          "targets": [
            {
              "expr": "{namespace=\"ingress-nginx\", container=\"controller\"} status>=500 and status<600 | stats by (_time:$__interval) count()",
              "queryType": "statsRange",
              "refId": "B"
            }
          ],
          "title": "5xx errors over time",
          "type": "timeseries"
        },
        {
          "datasource": {
            "type": "victoriametrics-logs-datasource",
            "uid": "victorialogs"
          },
          "gridPos": { "h": 6, "w": 24, "x": 0, "y": 6 },
          "id": 3,
          "options": {
            "legend": { "showLegend": true },
            "tooltip": { "mode": "single" }
          },
          "targets": [
            {
              "expr": "{namespace=\"ingress-nginx\", container=\"controller\"} status>=500 and status<600 _time:30m | stats by (request) count() as hits | sort by (hits desc) | limit 10",
              "queryType": "statsRange",
              "refId": "C"
            }
          ],
          "title": "Top 10 requests causing 5xx",
          "type": "table"
        },
        {
          "datasource": {
            "type": "victoriametrics-logs-datasource",
            "uid": "victorialogs"
          },
          "gridPos": { "h": 10, "w": 24, "x": 0, "y": 12 },
          "id": 4,
          "options": {
            "dedupStrategy": "none",
            "enableLogDetails": true,
            "showTime": true,
            "sortOrder": "Descending"
          },
          "targets": [
            {
              "expr": "{namespace=\"ingress-nginx\", container=\"controller\"} status>=500 and status<600",
              "queryType": "range",
              "refId": "D"
            }
          ],
          "title": "Raw ingress 5xx logs",
          "type": "logs"
        }
      ],
      "schemaVersion": 41,
      "title": "Ingress â€” 5xx Errors",
      "uid": "ingress-5xx",
      "version": 1
    }

===== ./dashboard/grafana-dashboard-victoriametrics-overview.yaml =====
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-dashboard-victoriametrics-overview
  namespace: ns-observability
  labels:
    grafana_dashboard: "1"
data:
  victoriametrics-overview.json: |
    {
      "title": "VictoriaMetrics â€” Overview",
      "uid": "vm-overview",
      "tags": ["victoriametrics"],
      "schemaVersion": 39,
      "version": 1,
      "timezone": "browser",
      "time": { "from": "now-1h", "to": "now" },
      "panels": [
        {
          "type": "stat",
          "title": "Active time series",
          "gridPos": {"x":0,"y":0,"w":6,"h":6},
          "datasource": {"type":"prometheus","uid":"victoriametrics"},
          "targets": [
            { "expr": "vm_active_time_series" }
          ]
        },
        {
          "type": "stat",
          "title": "Disk space used",
          "gridPos": {"x":6,"y":0,"w":6,"h":6},
          "datasource": {"type":"prometheus","uid":"victoriametrics"},
          "targets": [
            { "expr": "sum(vm_data_size_bytes)" }
          ]
        }
      ]
    }

===== ./dashboard/victoria1.yaml =====
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-dashboard-victorialogs-single
  namespace: ns-observability
  labels:
    grafana_dashboard: "1"
data:
  victorialogs-single-node.json: |
    {
      "id": null,
      "uid": "victorialogs-single",
      "title": "VictoriaLogs â€” Single Node",
      "tags": ["victorialogs","metrics"],
      "timezone": "browser",
      "schemaVersion": 39,
      "version": 2,
      "refresh": "30s",
      "panels": [
        {
          "type": "stat",
          "title": "Ingested log lines / s",
          "gridPos": {"x":0,"y":0,"w":8,"h":4},
          "datasource": {"type": "prometheus", "uid": "prometheus"},
          "targets": [
            {
              "expr": "rate(vl_log_ingested_lines_total[1m])",
              "legendFormat": "lines/s",
              "refId": "A"
            }
          ]
        },
        {
          "type": "stat",
          "title": "Ingested log bytes / s",
          "gridPos": {"x":8,"y":0,"w":8,"h":4},
          "datasource": {"type": "prometheus", "uid": "prometheus"},
          "targets": [
            {
              "expr": "rate(vl_log_ingested_bytes_total[1m])",
              "legendFormat": "bytes/s",
              "refId": "B"
            }
          ]
        },
        {
          "type": "stat",
          "title": "Active tenants",
          "gridPos": {"x":16,"y":0,"w":8,"h":4},
          "datasource": {"type": "prometheus", "uid": "prometheus"},
          "targets": [
            {
              "expr": "vl_active_tenants",
              "legendFormat": "tenants",
              "refId": "C"
            }
          ]
        },
        {
          "type": "timeseries",
          "title": "Query duration (p99)",
          "gridPos": {"x":0,"y":4,"w":24,"h":6},
          "datasource": {"type": "prometheus", "uid": "prometheus"},
          "targets": [
            {
              "expr": "histogram_quantile(0.99, sum(rate(vl_query_duration_seconds_bucket[5m])) by (le))",
              "legendFormat": "p99",
              "refId": "D"
            }
          ]
        },
        {
          "type": "timeseries",
          "title": "Query rate",
          "gridPos": {"x":0,"y":10,"w":24,"h":6},
          "datasource": {"type": "prometheus", "uid": "prometheus"},
          "targets": [
            {
              "expr": "rate(vl_queries_total[1m])",
              "legendFormat": "queries/s",
              "refId": "E"
            }
          ]
        }
      ]
    }

===== ./dashboard/grafana-dashboard-victoriametrics-errors.yaml =====
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-dashboard-victoriametrics-errors
  namespace: ns-observability
  labels:
    grafana_dashboard: "1"
data:
  victoriametrics-errors.json: |
    {
      "title": "VictoriaMetrics â€” Errors",
      "uid": "vm-errors",
      "tags": ["victoriametrics"],
      "schemaVersion": 39,
      "version": 1,
      "timezone": "browser",
      "time": { "from": "now-6h", "to": "now" },
      "panels": [
        {
          "type": "timeseries",
          "title": "Rejected samples",
          "gridPos": {"x":0,"y":0,"w":12,"h":6},
          "datasource": {"type":"prometheus","uid":"victoriametrics"},
          "targets": [
            { "expr": "rate(vm_rows_rejected_total[5m])" }
          ]
        },
        {
          "type": "timeseries",
          "title": "Failed requests",
          "gridPos": {"x":12,"y":0,"w":12,"h":6},
          "datasource": {"type":"prometheus","uid":"victoriametrics"},
          "targets": [
            { "expr": "rate(vm_http_request_errors_total[5m])" }
          ]
        }
      ]
    }

